{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN_IZKchZt6f",
        "colab_type": "text"
      },
      "source": [
        "# General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU-rk370aEFn",
        "colab_type": "text"
      },
      "source": [
        "I've learned that **artificial intelligence** and **machine learning** are more than what we think they are. **Artificial intelligence** is a broad term meaning \"the science of modeling intelligent machines.\" **Machine learning** is actually an overarching branch of ideas relating to a machine's capability of \"learning\" through experience of tasks, improving performance as it gains experience (as said by Tom Mitchell). A machine will receive input AND output and devise its own rules to explain how the output arose from the input. **Machine learning** can be divided into two categories:\n",
        "**Unsupervised learning** and **supervised learning**. **Unsupervised learning** is an aspect of M.L. where the data the machine has received is attributed with a label, so we are explicitly telling the machine that the data must be under some particular category. Here, we would affect the machine in a process called **training** to identify the properties that make each data fall into their respective categories. **Unsupervised learning** lacks this advantage, thereby leaving it to the machine's discretion to discover patterns in the data it receives. When it does so, its self-made rules will decide what goes where. Another branch, **reinforcement learning**, to be brief, involves rewarding certain behaviors and punishing undesired ones. It is most commonly used in the video game field of computation.\n",
        "\n",
        "**Deep learning** is the one most students in our class would frequently talk about. As another broad section of **machine learning**, they encompass another important aspect of teaching machines. Through **deep learning**, machines are taught through a process of using **deep neural networks**, computational models layered with nodes (**perceptrons**) that emulate the structure of our own brains' neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMUGBVWBZuEm",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSYK2pxFaEnO",
        "colab_type": "text"
      },
      "source": [
        "Within the branch of **deep learning** lies a popular tool that is shaping our world as we know it, known as **convolutional neural networks** (convnets). These networks are structured in multitudes of layers of **perceptrons**, meaning there is complete inter-connectedness between each **perceptron** in each layer of the convnet. The layers of a convnet are actually three-dimensional, bearing a length and width alongside a third dimension with a varied purpose (in the case of computer vision as was tested in this course, it may be used for entailing RGB for recognition of color). Within the layer are divisions meant to carry out tasks for processing the data. They have an **input layer** to receive the input, an **output layer** to receive the output, and a **hidden layer** (which a convnet may have multiple of) to pass transformed signals. It accomplishes this through its use of a **receptive field**.\n",
        "\n",
        "The convnet is itself rather complex and is still under intense study today. The components that define it are intricate, defining each layer in their own detailed manner. Some prominent components of the convnet are: **convolutional layers**, **pooling layers**, and **fully connected layers**. **Convolutional layers** are layers meant to inspect a small portion of the input layer (its **receptive field**) and compare it to the neuron that applies it. It processes it in a specified fashion to produce a numerical value as output, representing the comparison value. **Pooling layers** are significantly helpful with working with large images and \"pooling\" them to a more well-handled size for **convolutional layers**. They have many ways to do so, but **max pooling** is the most commonly used method. **Fully connected layers**, also known as **densely-connected layers**, accomplish classification tasks and regression tasks particularly well, realms that are not as well-suited for **convolutional layers**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn2zN12pZuJ-",
        "colab_type": "text"
      },
      "source": [
        "# Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNrVKma0aFhG",
        "colab_type": "text"
      },
      "source": [
        "When it comes to compiling a model, we assume the network's architecture is well-defined for smoothing the learning process. We tend to use a neural-network library called **Keras** that makes it much easier to create models for us. Two important keys to configure them are the **loss function** (also known as the **objective function**) and **optimizers**. The **loss function** describes what the quantity of a particular factor of the model will be minimized during the **training** process. In accordance with the types of problems one is facing and the **last-layer activation** of their choice, Keras auto-decides what the loss function expects the minimized factor would be (examples: binary_crossentropy, categorical_crossentropy, mse, and so on). **Optimizers**, in turn, is similar to the **loss function** when it comes to attempting to minimize \"loss\". However, it actually depends on the **loss function** and the parameters specified for our model to update the model itself thanks to the loss function's output. Through this, in a short yet sweet manner, it begins to gradually shape the model into what we hope to be correct, altering the weights of certain data as it sees fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KXXbiG2ZuPO",
        "colab_type": "text"
      },
      "source": [
        "# Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdR132RuaGQO",
        "colab_type": "text"
      },
      "source": [
        "The intricacies of **training** a model can get quite overbearing, as we've seen in previous homework assignments and examples shown in lecture. As a model is being shaped and influenced by the entirety of the content mentioned above, its self-made rules can be improved or worsened from our perspective. During **training**, a model is exposed to a variety of data, known as our **dataset**. A gradual process, the rules that our model form will dictate how it performs when exposed to the next **training data**. The use of **training** itself is a double-edged blade; the concept of **over-dependence** becomes readily obvious when our model begins to rely solely on the training data provided to it. As an example, if we attempt to train our model VERY extensively because we feel that it would help it be the best at predicting data, it can become over-reliant. When this happens, the model is rather poor at predicting data outside of the range of its **training data**. This is the purpose of using **testing data**, which is generally smaller than **training data** but is used for our model to \"think outside the box.\" If it scores very well on the **training** section but scores poor on the **testing** section, we have a clear-cut case of an **overfitted model**. The model is **overfit** if it is too dependent on the training data and cannot seem to do well on any separate data from the training one. For this reason, researchers or hobbyists will avoid causing such a scenario and watch the training intensity of the model. As with all things, there is a balance. Models can fall too far back on the opposite end of the spectrum and do poor overall on data prediction. If it is doing poorly on the **training data** (and implicitly, doing poorly on the **testing data**), it is considered an **underfitted model**. In this situation, it is not adapting well to the data it is receiving and its self-made rules are not explaining the data effectively enough. We as model creators need to maintain that balance so as to not fall in the trap of **overfitting** or **underfitting**, but the proper stage of **well-fit** models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTgmyFgZZuT2",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning a Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLaKidgKaG3m",
        "colab_type": "text"
      },
      "source": [
        "Assuming our models have finished their training and they are reasonable, we may find that particular kinks about them are unsufficient in our eyes. As such, we perform a process known as **fine-tuning** (altering of the model's parameters so as to adapt to new tasks it may be put through) to modify aspects of our model rather than redo its entire structure. We would alter certain layers we deem imperfect for the new task, which can be accomplished in a variety of ways. One such method (assuming **Keras** is used) is known as **layer-freezing**. We are specifically freezing the weights that correspond to a layer, which means that we would NOT modify this layer any further on any subsequent **training** sessions. The following code found through an online resource illustrates an example of **layer-freezing**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08grkT1UlMzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "#Load the VGG model\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Freeze the layers except the last 4 layers\n",
        "for layer in vgg_conv.layers[:-4]:\n",
        "    layer.trainable = False\n",
        " \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg_conv.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6a8spJ1lRXD",
        "colab_type": "text"
      },
      "source": [
        "Another such method is known as the use of **data generators**. In **Keras**, there exists a function called **ImageDataGenerator()** that is very beneficial in **data generation** (in which we select certain batches of data to perform some degree of data augmentation to our mode). From the same resource, it is illustrated below (not assuming the correct import statements are made):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1qkiOLmDbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 100\n",
        "val_batchsize = 10\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihjk3zRlmHxk",
        "colab_type": "text"
      },
      "source": [
        "Lastly, there is another well-known method in **Keras** known as **Flatten()** that can change the output of our model into a 1D array for improving our result's shape. The official website of **Keras**, keras.io, has an example usage of model-flattening below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpLLRVnImx1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load(open('bottleneck_features_train.npy'))\n",
        "# the features were saved in order, so recreating the labels is easy\n",
        "train_labels = np.array([0] * 1000 + [1] * 1000)\n",
        "\n",
        "validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
        "validation_labels = np.array([0] * 400 + [1] * 400)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_data, train_labels,\n",
        "          epochs=50,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(validation_data, validation_labels))\n",
        "model.save_weights('bottleneck_fc_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}